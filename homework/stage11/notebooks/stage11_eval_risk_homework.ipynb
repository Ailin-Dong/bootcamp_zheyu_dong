{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20b110da",
   "metadata": {},
   "source": [
    "# Stage 11 — Evaluation, Risk & Communication\n",
    "\n",
    "**Chain statement:** In the lecture, we learned to bootstrap uncertainty, compare scenarios, and communicate risks. Here we adapt that workflow to produce a validated baseline and stakeholder-facing summary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8918f7",
   "metadata": {},
   "source": [
    "## 0) Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0af9309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths — prefer your project path if exists; otherwise use bundled data\n",
    "USER_ROOT = Path(\"/Users/hust/bootcamp_zheyu_dong/homework/stage11\")\n",
    "PREFERRED = USER_ROOT / \"data\" / \"data_stage11_eval_risk.csv\"\n",
    "FALLBACK  = Path(\"/mnt/data/stage11/data/data_stage11_eval_risk.csv\")\n",
    "\n",
    "DATA_CSV = PREFERRED if PREFERRED.exists() else FALLBACK\n",
    "print(\"Using dataset:\", DATA_CSV)\n",
    "\n",
    "df = pd.read_csv(DATA_CSV, parse_dates=[\"date\"]).sort_values(\"date\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9402a8d",
   "metadata": {},
   "source": [
    "## 1) Baseline Fit & Metric (time-aware split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2987175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure time-aware split: last 25% as test\n",
    "cut = int(len(df) * 0.75)\n",
    "train_df = df.iloc[:cut].copy()\n",
    "test_df  = df.iloc[cut:].copy()\n",
    "\n",
    "features = [\"x1\",\"x2\"]\n",
    "target   = \"y\"\n",
    "\n",
    "X_train = train_df[features]\n",
    "y_train = train_df[target]\n",
    "X_test  = test_df[features]\n",
    "y_test  = test_df[target]\n",
    "\n",
    "print(\"Train/Test sizes:\", len(train_df), len(test_df))\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133630c7",
   "metadata": {},
   "source": [
    "### Pipelines & Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2320432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Scenario A: Mean imputation + scaling + LinearRegression\n",
    "pipe_mean = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "# Scenario B: Median imputation + scaling + LinearRegression\n",
    "pipe_median = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "# Scenario C (optional): PolynomialFeatures (degree=2) + mean impute + scaling + LinearRegression\n",
    "pipe_poly = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "pipes = { \"mean\": pipe_mean, \"median\": pipe_median, \"poly2\": pipe_poly }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246e8c35",
   "metadata": {},
   "source": [
    "### Fit, Predict, Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e949348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_pipe(name, pipe, X_train, y_train, X_test, y_test):\n",
    "    pipe.fit(X_train, y_train)\n",
    "    pred_train = pipe.predict(X_train)\n",
    "    pred_test  = pipe.predict(X_test)\n",
    "    rmse_tr = mean_squared_error(y_train, pred_train, squared=False)\n",
    "    rmse_te = mean_squared_error(y_test, pred_test, squared=False)\n",
    "    mae_tr  = mean_absolute_error(y_train, pred_train)\n",
    "    mae_te  = mean_absolute_error(y_test, pred_test)\n",
    "    res = {\n",
    "        \"name\": name, \"rmse_train\": rmse_tr, \"rmse_test\": rmse_te,\n",
    "        \"mae_train\": mae_tr, \"mae_test\": mae_te,\n",
    "        \"pred_train\": pred_train, \"pred_test\": pred_test,\n",
    "        \"resid_train\": y_train - pred_train, \"resid_test\": y_test - pred_test\n",
    "    }\n",
    "    return res\n",
    "\n",
    "results = {k: eval_pipe(k, p, X_train, y_train, X_test, y_test) for k, p in pipes.items()}\n",
    "pd.DataFrame([{k: v for k, v in r.items() if isinstance(v, (int, float))} for r in results.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c12297e",
   "metadata": {},
   "source": [
    "### Residual Diagnostics (baseline = mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5668e0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = results[\"mean\"]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10,8))\n",
    "\n",
    "# Residuals vs fitted (test)\n",
    "axes[0,0].scatter(base[\"pred_test\"], base[\"resid_test\"], alpha=0.5)\n",
    "axes[0,0].axhline(0, linewidth=1)\n",
    "axes[0,0].set_title(\"Residuals vs Fitted (test)\")\n",
    "axes[0,0].set_xlabel(\"Fitted\")\n",
    "axes[0,0].set_ylabel(\"Residuals\")\n",
    "\n",
    "# Histogram of residuals\n",
    "axes[0,1].hist(base[\"resid_test\"], bins=30)\n",
    "axes[0,1].set_title(\"Residual histogram (test)\")\n",
    "\n",
    "# Residuals over time\n",
    "axes[1,0].plot(test_df[\"date\"], base[\"resid_test\"])\n",
    "axes[1,0].axhline(0, linewidth=1)\n",
    "axes[1,0].set_title(\"Residuals over time (test)\")\n",
    "axes[1,0].set_xlabel(\"Date\")\n",
    "\n",
    "# True vs Pred\n",
    "axes[1,1].plot(test_df[\"date\"], y_test.values, label=\"truth\")\n",
    "axes[1,1].plot(test_df[\"date\"], base[\"pred_test\"], label=\"prediction\")\n",
    "axes[1,1].legend()\n",
    "axes[1,1].set_title(\"Prediction vs Truth (test)\")\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafa7ec5",
   "metadata": {},
   "source": [
    "## 2) Bootstrap — Uncertainty for RMSE & Prediction Band (baseline = mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ac8917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import bootstrap_model_metric, bootstrap_prediction_band, rmse\n",
    "\n",
    "boots = bootstrap_model_metric(pipes[\"mean\"], X_train, y_train, X_test, y_test, metric_fn=rmse, n_boot=500, random_state=0)\n",
    "boots_ci = (boots[\"ci_low\"], boots[\"ci_high\"])\n",
    "print(\"Bootstrap RMSE 95% CI:\", boots_ci)\n",
    "\n",
    "# Visualize RMSE distribution\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(boots[\"metrics\"], bins=30)\n",
    "plt.axvline(boots_ci[0], linestyle=\"--\")\n",
    "plt.axvline(boots_ci[1], linestyle=\"--\")\n",
    "plt.title(\"Bootstrap RMSE (test)\")\n",
    "plt.xlabel(\"RMSE\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Prediction band on test (quantiles per timestamp)\n",
    "band = bootstrap_prediction_band(pipes[\"mean\"], X_train, y_train, X_test, n_boot=300, random_state=0)\n",
    "q_lo, q_med, q_hi = band[\"quantiles\"]\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(test_df[\"date\"], y_test.values, label=\"truth\")\n",
    "plt.plot(test_df[\"date\"], q_med, label=\"pred median\")\n",
    "plt.fill_between(test_df[\"date\"], q_lo, q_hi, alpha=0.3, label=\"95% band\")\n",
    "plt.title(\"Prediction with Bootstrap Band (test)\")\n",
    "plt.legend()\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0642ed1d",
   "metadata": {},
   "source": [
    "## 3) Scenario Comparison — Mean vs Median Imputation (Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f87a683",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12,4), sharey=True)\n",
    "for ax, key in zip(axes, [\"mean\",\"median\"]):\n",
    "    r = results[key]\n",
    "    ax.plot(test_df[\"date\"], y_test.values, label=\"truth\")\n",
    "    ax.plot(test_df[\"date\"], r[\"pred_test\"], label=\"pred\")\n",
    "    ax.set_title(f\"{key} imputation — RMSE={r['rmse_test']:.3f}\")\n",
    "    ax.legend()\n",
    "axes[0].set_ylabel(\"y\")\n",
    "plt.suptitle(\"Scenario: Mean vs Median (Linear Regression)\", y=1.02)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f92f5c",
   "metadata": {},
   "source": [
    "## 4) Scenario Comparison — Linear vs PolynomialFeatures (degree=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a009150",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12,4), sharey=True)\n",
    "for ax, key in zip(axes, [\"mean\",\"poly2\"]):\n",
    "    r = results[key]\n",
    "    ax.plot(test_df[\"date\"], y_test.values, label=\"truth\")\n",
    "    ax.plot(test_df[\"date\"], r[\"pred_test\"], label=\"pred\")\n",
    "    ax.set_title(f\"{key} — RMSE={r['rmse_test']:.3f}\")\n",
    "    ax.legend()\n",
    "axes[0].set_ylabel(\"y\")\n",
    "plt.suptitle(\"Scenario: Linear vs Polynomial (degree=2)\", y=1.02)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe46d4d4",
   "metadata": {},
   "source": [
    "## 5) Subgroup Diagnostics — by Segment (A/B/C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518c1df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach predictions from baseline to test_df for subgroup comparison\n",
    "test_sub = test_df.copy()\n",
    "test_sub[\"pred\"] = results[\"mean\"][\"pred_test\"]\n",
    "test_sub[\"resid\"] = results[\"mean\"][\"resid_test\"]\n",
    "\n",
    "# Metrics per segment\n",
    "rows = []\n",
    "for seg, g in test_sub.groupby(\"segment\"):\n",
    "    rows.append({\n",
    "        \"segment\": seg,\n",
    "        \"rmse\": mean_squared_error(g[\"y\"], g[\"pred\"], squared=False),\n",
    "        \"mae\": mean_absolute_error(g[\"y\"], g[\"pred\"]),\n",
    "        \"n\": len(g)\n",
    "    })\n",
    "seg_table = pd.DataFrame(rows).sort_values(\"segment\")\n",
    "display(seg_table)\n",
    "\n",
    "# Visuals: residual box by segment & bar of RMSE\n",
    "fig, axes = plt.subplots(1,2, figsize=(10,4))\n",
    "\n",
    "# Boxplot using matplotlib\n",
    "segments = seg_table[\"segment\"].tolist()\n",
    "data = [test_sub.loc[test_sub[\"segment\"]==s, \"resid\"].values for s in segments]\n",
    "axes[0].boxplot(data, labels=segments)\n",
    "axes[0].set_title(\"Residuals by Segment (test)\")\n",
    "axes[0].set_xlabel(\"segment\")\n",
    "axes[0].set_ylabel(\"residual\")\n",
    "\n",
    "# Bar chart RMSE\n",
    "axes[1].bar(seg_table[\"segment\"], seg_table[\"rmse\"])\n",
    "axes[1].set_title(\"RMSE by Segment (test)\")\n",
    "axes[1].set_xlabel(\"segment\")\n",
    "axes[1].set_ylabel(\"RMSE\")\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6780f574",
   "metadata": {},
   "source": [
    "## 6) Stakeholder Summary (≤ 1 page)\n",
    "\n",
    "**Assumptions**\n",
    "- Relationship is approximately linear (or captured by degree=2 terms). \n",
    "- Missing values are at random; imputation (mean/median) is appropriate.\n",
    "- Recent dynamics stable enough that training window generalizes to the last 25% (test).\n",
    "\n",
    "**Risks**\n",
    "- Structural breaks/seasonality shifts would hurt accuracy; fixed-window features may go stale.\n",
    "- Non-random missingness could bias imputation; try robust imputers or model-based imputation.\n",
    "- Segment C shows higher error (see subgroup); decisions on C should use wider error margins.\n",
    "\n",
    "**Sensitivity**\n",
    "- Bootstrap RMSE 95% CI quantifies uncertainty in test performance.\n",
    "- Mean vs median imputation yields similar trends; choose based on robustness to skew/outliers.\n",
    "- Polynomial features can reduce bias if mild curvature exists; watch for variance increase.\n",
    "\n",
    "**Takeaway**\n",
    "- Baseline is serviceable and reasonably stable within the CI band on test. \n",
    "- For production decisions: monitor segment-specific error; re-train if error band widens or drift detected.\n",
    "- Next: tune regularization, add seasonality/interaction terms, and consider robust models if heteroscedasticity persists."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
